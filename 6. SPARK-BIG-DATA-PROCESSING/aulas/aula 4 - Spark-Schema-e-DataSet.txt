Spark Schema

DataFrame - Criação de Schemas

Tipagem de Dados
oSpark SQL e DataFrame

| ByteType    | StringType    |
| ----------- | ------------- |
| ShortType   | BinaryType    |
| IntegerType | BooleanType   |
| LongType    | TimestampType |
| FloatType   | DateType      |
| DoubleType  | ArrayType     |
| DecimalType | MapType       |
| BigDecimal  | StructType    |

ohttps://spark.apache.org/docs/latest/sql-ref-datatypes.html

Esquemas(estrutura dos dados) - Definição

oInferir esquema manualmente em dados com cabeçalho. Inferir Squema com Spark X Criar Squema manualmente. Inferir Squema ele precisa ler um conjunto de dados para processar e dizer quais os tipos de dados contém no Squema, e sempre jogando pra cima, isso aumenta o uso de memória.

setor.csv
id, setor
1, vendas
2, TI
3, RH

Scala

scala>import org.apache.spark.sql.types._ // Caso não queiram especificar os tipos de dados, inserindo o _, ele habilitará todos os tipos para serem usados.
scala> val columnsList = List(
				StructField("id", IntegerType),
				StructField("setor", StringType) )
scala> val setorSchema = StructType(columnsList)
scala> val setorDF = spark.read.option("header","true").schema(setorSchema).csv("setor.csv")

Python
oInferir esquema manualmente em dados com cabeçalho

$ from pyspark.sql.types import *
$ columns_list = [
StructField("id", StringType() ),
StructField("setor", StringType() )]
$ setor_schema = StructType(columns_list)
$ setor_df = spark.read.option("header","true").schema(setor_schema).csv("setor.csv")

**DataFrame - Testar Schemas**

Python 
$ from pyspark.sql.types import *
$ columns_list = [
StructField("id", StringType() ),
StructField("setor", StringType() )]
$ setor_schema = StructType(columns_list)
$ dados_teste = [Row(1, vendas "), Row(2,"TI"), Row(3,'RH'))]
$ setor_df = spark.createDataFrame(data = dados_teste, schema=setor_schema)

oPython Row class
from pyspark. sql import Row
Schema = Row("id","setor")
dados_teste = [ Schema(1, vendas "), Schema(2,"TI"), Schema(3,'RH')]
teste_df = spark.createDataFrame (data= dados_teste)
teste_df.printSchema()

------

**DataSet**

**DataSet - Conceitos**

oColeção distribuída de objetos de tipagem forte. Por ser de tipagem forte, é usado em linguagens fortemente tipadas como Java e Scala.
•Tipos primitivos: Int ou String
•Tipos Complexos: Array ou listas
•Product objects
oScala case classes
oJava JavaBen objects
•Row objects
oMapeado para um schema relacional
•Schema é definido por um encoder
•Schema mapea objeto de propriedades para tipos de colunas
oOtimizada pelo Catalyst
oImplementado apenas em Java e Scala
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html
https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html

DataFrame x Dataset
oDataFrames (Conjuntos de dados de Row objects)
	•**Representam dados tabulares**
	•Transformações são não tipadas
		oLinhas podem conter elementos de qualquer tipo
		oSchemas definidos não são aplicados os tipos de coluna até o tempo de execução

oDatasets
	•Representam dados tipados e orientados a objeto
	•Transformações são tipadas
		oPropriedades do objeto são tipadas em tempo de compilação. Com isso é possível ganhar perfomance e identificar erros na leitura dos dados ou na hora de aplicar as transformações.
oRepresentação dos dados
	•RDD - 2011 - Varrer o bit, fazer map, flatmap, etc.
	•Dataframe - 2013 - Já lida com os dados semi-estruturados, estruturados, por isso já ficou mais fácil.
	•Dataset - 2015 - Por de baixo é um RDD, mas tem todas as vantagens de um Dataframe , porém, em tempo de compilação, contendo as melhores características dos formatos de RDD e Dataframe. 

**Criação de DataSet**

oCriar Dataset com case classes ( Recomendada
	•case class <NomeClasse>(<Atributo>:<Tipo>, ..., <Atributo>:<Tipo>)
oExemplo
case class Name(id: Integer, name: String)
reg = Seq(Name(1,"Rodrigo"),Name(2,"Augusto"))
regDS = spark.createDataset(reg)
regDS.show
//Imprimir para cada linha só o name
regDS.foreach(n => println(n.name))

oCriação de Dataset de DataFrame

oLer dados estruturados para um DataFrame
oCriar um Dataset para ler os dados do DataFrame
oForçar para inserir um schema com o Encoder
case class Name(id: Integer, name: String)
val regDF = spark.read.json registros.json
val regDS = regDF.as[Name] //Name é o case class
regDS.show

import org.apache.spark.sql.Encoders
val schema = Encoders.product[Name].schema
val regDS = spark.read.schema(schema).json("registros.json").as[Name]

oCriação de Dataset de RDD
oLer dados não estruturados ou semi estruturados para um RDD
oCriar um Dataset para ler os dados do RDD
case class PcodeLatLon(pcode: String, latlon: Tuple2[Double,Double])
val pLatLonRDD = sc.textFile("latlon.tsv").map(_.split('t’)).map(fields =>(PcodeLatLon(fields(0),(fields(1).toFloat,fields(2).toFloat))))
val pLatLonDS = spark.createDataset(pLatLonRDD)
pLatLonDS.printSchema
println(pLatLonDS.first)

**Transformações de Dataset**

oTransformações tipadas criam um novo Dataset
•Filter
•Limit
•Sort
•flatMap
•Map
•orderBy

oTransformações não tipadas retornam DataFrames ou colunas não tipadas
•Join
•groupBy
•Col
•Drop
•Select
•withColumn

Exemplo de transformações
oTipadas: Dataset
oNão tipadas: DataFrame

scala> val sortedDS = regDS.sort("name")
*sortedDS: org.apache.spark.sql.Dataset[Name] = [id: int, name: string]*
scala> val nameDF = regDS.select("name")
*nameDF: org.apache.spark.sql.DataFrame = [name: string]*
scala>val combineDF = regDS.sort(“name").where("id > 10").select(“name")
*combineDF: org.apache.spark.sql.DataFrame = [name: string]*

Salvar Dataset
oSão salvos como DataFrames
	•Dataset.write
		oRetorna um DataFrameWriter
•Exemplo
regDS.write.save("hdfs://localhost/user/cloudera/registros/")
regDS.write.json("registros")

